import json
import os
import re
import argparse

BENCHMARK_RESULTS_FILE = "output/benchmark_results.json"
OUTPUT_TEX_FILE = "output/solutions_report.tex"

def escape_latex(text):
    """
    Escapes special LaTeX characters in a given string.
    """
    if not isinstance(text, str):
        return ""
    return re.sub(r'([&%$#_{}])', r'\\\1', text)

TEX_TEMPLATE_HEADER = r"""
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{array}
\usepackage{lmodern}
\usepackage{fancyvrb}

\geometry{a4paper, margin=1in}

\definecolor{correct}{HTML}{28a745}
\definecolor{partial}{HTML}{FFC107}
\definecolor{incorrect}{HTML}{DC3545}
\definecolor{noerror}{HTML}{6C757D}

\title{LLM Benchmark Report}
\author{Generated by script}
\date{\today}

\begin{document}
\maketitle
"""

TEX_TEMPLATE_FOOTER = r"""
\end{document}
"""

def generate_summary_table(summary_data):
    """Generates the LaTeX for the summary statistics table."""
    if not summary_data:
        return ""

    table_header = r"""
\section*{Summary Statistics}
\begin{center}
\begin{longtable}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Total} & \textbf{Correct} & \textbf{Partial} & \textbf{Incorrect} & \textbf{Errors} & \textbf{Avg. Score} \\
\hline
\endfirsthead
\hline
\endfoot
"""
    table_rows = ""
    for model, stats in summary_data.items():
        table_rows += f"{escape_latex(model)} & {stats.get('total', 0)} & {stats.get('1_count', 0)} & {stats.get('0.5_count', 0)} & {stats.get('0_count', 0)} & {stats.get('null_count', 0)} & {stats.get('average_score', 0.0):.2f} \\\\\n"

    table_footer = r"""\hline
\end{longtable}
\end{center}
"""
    return table_header + table_rows + table_footer


def generate_problems_section(results_data):
    """Generates the LaTeX for the detailed problems section."""
    problems_tex = "\\section*{Problem Details}\n\n"
    
    for i, problem in enumerate(results_data):
        paper_id = escape_latex(problem.get("paper_id", "N/A"))
        problem_statement = problem.get("problem_statement", "") # This should be valid LaTeX
        ground_truth_solution = problem.get("ground_truth_solution", "") # This should be valid LaTeX

        problems_tex += f"\\subsection*{{Problem {i+1} (Paper: {paper_id})}}\n"
        problems_tex += "\\subsubsection*{Problem Statement}\n"
        problems_tex += f"{problem_statement}\n\n"
        
        problems_tex += "\\subsubsection*{Ground Truth Solution}\n"
        # Handle the ground truth solution more carefully
        if ground_truth_solution.strip():
            solution = ground_truth_solution.strip()
            
            # Special handling for the truncated solution in problem 8
            if "and where all functional derivatives" in solution and solution.endswith("\\boldsymbol{B"):
                solution += ")} \\]"
            
            # If the solution already contains \boxed, don't double-box it
            if "\\boxed{" in solution:
                problems_tex += f"\\[ {solution} \\]\n\n"
            else:
                problems_tex += f"\\[ \\boxed{{{solution}}} \\]\n\n"
        else:
            problems_tex += "No ground truth solution provided.\n\n"
        
        problems_tex += "\\subsubsection*{Model Outputs}\n"
        
        for model_name, output in problem.get("model_outputs", {}).items():
            score = output.get('score')
            color = "noerror"
            if score == 1.0: color = "correct"
            elif score == 0.5: color = "partial"
            elif score == 0.0: color = "incorrect"
            
            score_str = str(score) if score is not None else "N/A"
            
            problems_tex += f"\\subsubsection*{{Model: {escape_latex(model_name)} (Score: {score_str})}}\n"
            
            # Model solution may contain LaTeX, so wrap it in math mode if needed
            model_solution = output.get("solution", "No solution provided.")
            problems_tex += "\\paragraph*{Model Solution:}\n"
            # Check if the solution looks like it should be in math mode
            if model_solution.strip() and not model_solution.strip().startswith("No solution"):
                # Wrap in display math mode
                problems_tex += f"\\[ {model_solution} \\]\n\n"
            else:
                problems_tex += f"{model_solution}\n\n"
            
            # Judge evaluation is plain text
            evaluation = output.get("evaluation", "No evaluation provided.")
            problems_tex += "\\paragraph*{Judge's Evaluation:}\n"
            problems_tex += f"\n{evaluation}\n\n"
            
        problems_tex += "\\newpage\n"

    return problems_tex


def export_benchmark_to_tex(benchmark_results_file, output_tex_file):
    """
    Reads benchmark results and generates a LaTeX report.
    """
    if not os.path.exists(benchmark_results_file):
        print(f"Error: Benchmark results file not found at '{benchmark_results_file}'")
        return

    with open(benchmark_results_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # Generate content
    summary_table_tex = generate_summary_table(data.get("summary", {}))
    problems_section_tex = generate_problems_section(data.get("results", []))

    # Combine all parts
    final_tex = TEX_TEMPLATE_HEADER
    final_tex += summary_table_tex
    final_tex += problems_section_tex
    final_tex += TEX_TEMPLATE_FOOTER

    # Write to file
    with open(output_tex_file, 'w', encoding='utf-8') as f:
        f.write(final_tex)

    print(f"LaTeX report successfully generated at '{output_tex_file}'")
    print("You can now compile this file using a LaTeX distribution (like pdflatex) to create a PDF.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Export benchmark results to LaTeX")
    parser.add_argument("--benchmark-results-file", type=str, default=BENCHMARK_RESULTS_FILE, help="Path to the benchmark results file")
    parser.add_argument("--output-tex-file", type=str, default=OUTPUT_TEX_FILE, help="Path to the output LaTeX file")
    args = parser.parse_args()

    export_benchmark_to_tex(args.benchmark_results_file, args.output_tex_file) 