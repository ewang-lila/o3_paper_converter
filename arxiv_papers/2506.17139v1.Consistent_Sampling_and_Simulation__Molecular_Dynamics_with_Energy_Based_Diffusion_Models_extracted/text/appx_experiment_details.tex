\section{Details for Experiments} \label{appx:details}
\subsection{Metrics} \label{appx:metrics}
To compute the \gls{JS} divergence and the \gls{PMF} error, we first discretize the observed free energy into binned histograms. For the \gls{JS} divergence, we then compute the \gls{JS} distance between the two probability vectors (we flatten the 2D histograms). To prevent discontinuities, we assume that in each bin there is at least one observation by adding 1. 

As for the \gls{PMF} error, we discretize into 64 bins and compute the proportion of samples in each window. These are then transformed by taking the $\log$ in each bin and then computing the square loss, which is averaged over all bins. Similarly, we have ensured that each bin contains some data and have added $10^{-6}$ as a baseline proportion. 
The approach and implementation are analogous to \cite{durumeric2024learning}.

\subsection{Toy System} \label{appx:mueller-brown-details}
\textbf{Dataset.} We have used a version of the Müller-Brown potential \citep{muellerbrown1979} to demonstrate the capabilities of our approach in two dimensions. For this, we have used the following potential
\begin{equation}
    \begin{aligned}
    U(x, y) = & -200 \cdot \exp \left( -(x-1)^2 -10 y^2 \right) \\
              & -100 \cdot \exp \left( -x^2 - 10 \cdot (y - 0.5)^2 \right) \\
              & -170 \cdot \exp \left( -6.5 \cdot (0.5 + x)^2 + 11 \cdot (x +0.5) \cdot (y -1.5) -6.5 \cdot (y -1.5)^2 \right) \\
              & + \phantom{0}15 \cdot \exp \left( 0.7 \cdot (1 + x)^2 +0.6 \cdot (x + 1) \cdot (y -1) +0.7 \cdot (y -1)^2 \right) \, .
    \end{aligned}
\end{equation}
To generate training samples from this potential, we have performed a Langevin simulation (compare \cref{eq:langevin}). For this, we have performed 5M steps with $k_BT=23, dt=0.005, \bM = 0.5 \cdot I$, where we only store every 50th sample to generate 100k training samples.

\textbf{Architecture and training.} For the toy systems, we have used a simple multi-layer perception with the hyperparameters presented in \cref{tab:mueller-brown-hyperparameters}. As for the optimizer, we have used AdamW \citep{loshchilov2019adamw}.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l c c c c }
        \toprule
        \textbf{Parameter} & \textbf{Diffusion} & \textbf{Mixture} & \textbf{Fokker-Planck} & \textbf{Both} \\\midrule
         \# Parameters & 17849 & 17263 & 17849 & 17263 \\
         BS & 128 & 128 & 128 & 128 \\ 
         Model-Ranges & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) \\
         Epochs & 180 & 120, 30, 30 & 180 & 120, 30, 30 \\
         Hidden Layers & [92, 92, 92] & [64, 64, 64], [64, 64], [54, 54]  & [92, 92, 92] & [64, 64, 64], [64, 64], [54, 54] \\
         $\alpha$ & 0 & 0 & 0.0005 & 0.0005, 0, 0 \\
         \bottomrule
    \end{tabular}
    }
    \vspace{0.2cm}
    \caption{This table contains the hyperparameters for the different models shown for the Müller-Brown potential.}
    \label{tab:mueller-brown-hyperparameters}
\end{table}

\subsection{Formalization Coarse-Craining}
In coarse-graining, we aim to reduce the number of dimensions of our system by combining multiple atoms into individual beads. Given non-\gls{CG} samples $\x$, the Boltzmann distribution of \gls{CG} samples $\z$ can be  recovered by $\prob(\z) \propto \int \exp \left( -\frac{\potential(\x)}{k_BT} \right) \delta (\Xi(\x) - \z) \d{\x}$ which defines the \gls{CG} potential up to a constant. $\delta$ is the Dirac delta function.

\subsection{Alanine Dipeptide} \label{appx:details-dataset-aldp}
\textbf{Dataset.}
The alanine dipeptide datasets is available as part of the public \texttt{bgmol}  (MIT licence) repository here: \url{https://github.com/noegroup/bgmol}.  
The dataset was generated with an MD simulation, using the classical \textit{Amber ff99SBildn} force-field at $300\textrm{K}$ for implicit solvent for a duration of $1\textrm{ms}$ \cite{kohler2021smooth} with the \texttt{openMM} library \citep{eastman2017openmm}. For training, we have selected 50k random samples from this simulation.

\textbf{Architecture.} For alanine dipeptide we have used quite a small architecture, where the hyperparameters are listed in \cref{tab:aldp-hyperparameters}. When multiple parameters are listed for the same model, this means that they are used for the corresponding \gls{MoE} model. Note that when using \gls{MoE}, we have mostly used the same model architecture, except that only the Fokker-Planck regularized model is conservative.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l c c c c }
        \toprule
        \textbf{Parameter} & \textbf{Diffusion} & \textbf{Mixture} & \textbf{Fokker-Planck} & \textbf{Both} \\\midrule
         Epochs & 10000 & 7000, 2000, 1000 & 10000 & 7000, 2000, 1000 \\
         BS & 1024 & 1024 & 1024 & 1024 \\ 
         Attention Heads & 8 & 8 & 8 & 8 \\
         Feature Dim & 16 & 16 & 16 & 16 \\
         Model-Ranges & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) \\
         Conservative & Yes & Yes, No, No & Yes & Yes, No, No \\
         $\alpha$ & 0 & 0, 0, 0 & 0.0005 & 0.0001, 0, 0 \\
         Hidden Dimension & 96 & 96 & 96 & 96 \\
         Layers & 2 & 2 & 2 & 2 \\
         \bottomrule
    \end{tabular}
    }
    \vspace{0.2cm}
    \caption{This table contains the hyperparameters for the different models shown for alanine dipeptide.}
    \label{tab:aldp-hyperparameters}
\end{table}

\textbf{Simulation.} To perform Langevin simulation, we have extracted the forces from the model via \cref{eq:score-equals-forces} at $t=10^{-5}$ for all models except for \emph{Two For One}, where we chose $t=0.02$, the same hyperparameter as presented in \cite{arts2023}.

\subsection{Dipeptides (2AA)} \label{appx:details-dataset-2aa}
\textbf{Dataset.}
The original dipeptide dataset (2AA) was introduced in \cite{klein2023timewarp} (MIT License) and is available here: \url{https://huggingface.co/datasets/microsoft/timewarp}. As this includes a lot of intermediate saved states and quantities, like energies, there is a smaller version made available by \citet{klein2024tbg} (CC BY 4.0):  \url{https://osf.io/n8vz3/?view_only=1052300a21bd43c08f700016728aa96e}. 
For a comprehensive overview of the simulation details, refer to \cite{klein2023timewarp}. All dipeptides were simulated in implicit solvent with a classical \textit{amber-14} force-field at $T=310$K. The simulation of the training and validation peptides were run for $50\textrm{ns}$, while the test peptides were simulated for $1\mu\textrm{s}$. All simulation were performed with the \texttt{openMM} library \citep{eastman2017openmm}.

Note that we have removed dipeptides containing Glycine from our dataset to ensure that all dipeptides have the same number of (coarse-grained) atoms. This made it easier to handle it in the code, but it is not a technical limitation of our architecture. It is split into 175 train, 75 validation, and 92 test dipeptides, out of which we have used 15 for the results in the paper (also the metrics) to reduce on inference time. 

\textbf{Architecture.} The hyperparameters are listed in \cref{tab:minipeptide-hyperparameters}. When multiple parameters are listed for the same model, this means that they are used for the corresponding \gls{MoE} model. Note that when using \gls{MoE}, we have used smaller networks for larger diffusion times, and only the Fokker-Planck regularized model is conservative.

\begin{table}[h]
    \centering
    \scalebox{0.8}{
    \begin{tabular}{l c c c c }
        \toprule
        \textbf{Parameter} & \textbf{Diffusion} & \textbf{Mixture} & \textbf{Fokker-Planck} & \textbf{Both} \\\midrule
         Epochs & 120 & 100, 20, 10 & 120 & 100, 20, 10 \\
         BS & 1024 & 1024 & 1024 & 1024 \\ 
         Attention Heads & 8 & 8 & 8 & 8 \\
         Feature Dim & 16 & 16 & 16 & 16 \\
         Model-Ranges & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) & (0, 1) & (0, 0.1), [0.1, 0.6), [0.6, 1.0) \\
         Conservative & Yes & Yes, No, No & Yes & Yes, No, No \\
         $\alpha$ & 0 & 0, 0, 0 & 0.0005 & 0.0001, 0, 0 \\
         Hidden Dimension & 128 & 128, 96, 96 & 128 & 128, 96, 96 \\
         Layers & 3 & 3, 2, 2 & 3 & 3, 2, 2 \\
         \bottomrule
    \end{tabular}
    }
    \vspace{0.2cm}
    \caption{This table contains the hyperparameters for the different models shown for the minipeptides.}
    \label{tab:minipeptide-hyperparameters}
\end{table}

\textbf{Simulation.} To perform Langevin simulation, we have extracted the forces from the model via \cref{eq:score-equals-forces} at $t=10^{-5}$ for all models except for \emph{Two For One}. As this system has not been tested by \cite{arts2023}, we opted to use the same $t$ as for Alanine dipeptide, namely $t=0.02$, which yielded robust results.

\subsection{Compute Infrastructure} \label{appx:compute-resources}
We have used a single RTX 3090 GPU for the toy systems, an A100 with 80GB memory for alanine dipeptide, and two A100 80GB GPUs for the dipeptide dataset.

\subsection{Software Licences} \label{appx:licenses}
In our code, we have used \texttt{jax} \citep{jax2018github} (Apache-2.0) and the accompanying machine learning library \texttt{flax} \citep{flax2020github} (Apache-2.0). For the graph transformer architecture, we have extended code from \cite{arts2023} (MIT) and have re-implemented the code from \url{https://github.com/lucidrains/graph-transformer-pytorch} (MIT) in \texttt{jax}.

For the free-energy plots of the Müller-Brown potential, we used \cite{hoffmann2021deeptime} (LGPL-3.0). For trajectories and simulations, we have used \texttt{openMM} \citep{eastman2017openmm} (MIT) and \texttt{mdtraj} \citep{McGibbon2015MDTraj} (LGPL-2.1).