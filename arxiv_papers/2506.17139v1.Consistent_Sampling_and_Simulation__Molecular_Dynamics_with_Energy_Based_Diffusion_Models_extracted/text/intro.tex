\section{Introduction}
Methodological advancements and increasing computational resources have allowed \gls{MD} simulations to reach biologically relevant timescales \citep{deshaw20211fastfolding, wolf2020}. However, scaling \gls{MD} to larger or slower-changing systems remains computationally challenging. \Gls{CG} methods address this by reducing the system dimensionality, but this comes at the cost of physical resolution, making it impossible to model interactions using traditional force fields accurately. Learning-based approaches \citep{clementi2008coarse,noid2013perspective,husic2020coarse, charron2023navigating}, offer an alternative by approximating these coarse-grained interactions.

Diffusion models \citep{ho2020diffusion, song2021} have recently demonstrated considerable success in various molecular tasks, including on proteins and larger systems, usually relying on some form of coarse-graining \citep{abramson2024alphafold3, watson2023rfdiffusion, corso2023diffdock, plainer2023diffdockpocket, lewis2024scalable}. The idea behind diffusion models is to learn a reverse stochastic process that removes noise. Starting from pure noise, the model iteratively denoises the samples until it resembles the data distribution, and the neural network models the so-called \emph{score} of the training data $\nabla_x \log \prob$.

When the training data accurately reflects samples from the equilibrium distribution of molecules, the learned score can be used not only for classical independent diffusion sampling, but also for \gls{MD} simulations \citep{arts2023}, providing access to thermodynamic and kinetic properties beyond static distributions. However, extracting the score from diffusion models (or the energy $\log \nnetprob$ for that matter) does not work well in practice, even for low-dimensional toy systems \citep{koehler2023statistical, li2023generalization}. While small local inaccuracies have little effect on independent sampling in diffusion models, using the extracted model for energy estimation reveals inconsistencies that can accumulate. 

Analogously, while diffusion models should satisfy the Fokker-Planck equation \citep{saarka2019}, previous work shows that existing diffusion models violate this condition \citep{lai2023}, especially when evaluated close to the data distribution. We hypothesize, and subsequently show empirically, that enforcing the Fokker-Planck equation significantly improves the consistency of the learned energy $\log \nnetprob$ and with it the alignment between independent samples and long-running simulations. We can see this behavior demonstrated on a two-dimensional toy example in \cref{fig:main}.

To implement this, we parameterize the score as the gradient of an energy function to ensure the learned score is conservative, and we have access to the energy $\log \nnetprob$. With this, we can introduce a Fokker-Planck-based regularization that minimizes deviations from theoretical consistency. However, directly evaluating the Fokker-Planck equation requires costly divergence computations, so we derive a computationally efficient \enquote{weak} residual formulation that requires only first-order derivatives. By further partitioning the diffusion timeline into distinct intervals handled by separate models, we can selectively apply the regularization only to the high error regions. This allows the model to learn to focus on the details and reduces training and inference costs. In \cref{sec:experiments}, we validate our approach on a toy system, alanine dipeptide, and demonstrate its scalability by training a transferable Boltzmann emulator across dipeptides.

Our main contributions in this work are as follows:
\begin{enumerate}
    \item We show how to regularize the energy of diffusion models using the Fokker-Planck equation, enabling consistent molecular dynamics simulations alongside traditional sampling.
    \item We demonstrate that training on a small sub-interval of the diffusion process suffices for stable simulation. By combining this with smaller models trained on complementary intervals, we achieve efficient training and inference without sacrificing sampling performance.
    \item We develop a state-of-the-art transferrable Boltzmann emulator for dipeptides capable of high-quality independent sampling and consistent simulation. 
\end{enumerate}


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.93]{figures/main}
    \caption{Training diffusion models on a simple mixture of two Gaussians reveals inconsistencies. While classical iid diffusion sampling recovers both modes, when estimating the unnormalized density at $\t=0$, we observe that the model learns a third mode and an incorrect mass distribution. Using this model for simulation produces invalid results. Introducing our Fokker-Planck regularization makes the model more self-consistent. }
    \label{fig:main}
\end{figure}
